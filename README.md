# nlp-pretrained-model-paper-review


|Year|Paper|Links|
|:---:|:---:|:---:|
|`2018/10`|`BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding`|[Paper](https://arxiv.org/pdf/1810.04805.pdf)<br>[Review](https://eunsour.tistory.com/103)|
