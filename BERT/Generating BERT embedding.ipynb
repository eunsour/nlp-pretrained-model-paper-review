{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b449f020",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-13T08:01:52.622686Z",
     "start_time": "2022-11-13T08:01:52.615252Z"
    }
   },
   "source": [
    "```python3\n",
    "pip install -q sentencepiece\n",
    "pip install -q transformers\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f433ba0",
   "metadata": {},
   "source": [
    "# BERT 임베딩 생성하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0892aeb",
   "metadata": {},
   "source": [
    "- 사전 학습된 BERT 모델을 다운로드 한다. \n",
    "- bert-base-uncased 모델은 12개의 인코더가 있는 모두 소문자로 변환한 uncased 토큰으로 변환된 BERT 기반 모델이다. \n",
    "- 표현 벡터 크기는 768이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4fbdc52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T14:42:19.959070Z",
     "start_time": "2022-11-14T14:42:16.882881Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9813b814",
   "metadata": {},
   "source": [
    "- bert-base-uncased 모델을 사전 학습 시키는데 사용된 토크나이저를 다운로드한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc5b9a06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T14:42:21.485360Z",
     "start_time": "2022-11-14T14:42:20.653608Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6594931b",
   "metadata": {},
   "source": [
    "**입력 전처리하기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d3b4982",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T14:42:21.762483Z",
     "start_time": "2022-11-14T14:42:21.759571Z"
    }
   },
   "outputs": [],
   "source": [
    "sentence = 'I love Korea'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a93b148",
   "metadata": {},
   "source": [
    "문장을 토큰화하고 토큰을 얻는다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b282c351",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T14:42:22.604962Z",
     "start_time": "2022-11-14T14:42:22.596637Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'love', 'korea']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b391ee",
   "metadata": {},
   "source": [
    "시작 부분에 [CLS] 토큰을 추가하고 토큰 목록 끝에 [SEP] 토큰을 추가한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b76b45dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T14:42:23.040953Z",
     "start_time": "2022-11-14T14:42:23.033866Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'i', 'love', 'korea', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8ac06e",
   "metadata": {},
   "source": [
    "토큰 목록의 길이를 7로 유지해야 한다고 가정하면 끝이 2개의 [PAD] 토큰을 추가해야 한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6288fb4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T14:42:23.397421Z",
     "start_time": "2022-11-14T14:42:23.392548Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'i', 'love', 'korea', '[SEP]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokens + ['[PAD]'] + ['[PAD]']\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f47b66",
   "metadata": {},
   "source": [
    "토큰이 [PAD] 토큰이 아니면 어텐션 마스크 값을 1로 설정하고, 그렇지 않으면 0으로 채운다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "464a74d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T14:42:23.745124Z",
     "start_time": "2022-11-14T14:42:23.739761Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "attention_mask = [1 if i!= '[PAD]' else 0 for i in tokens]\n",
    "print(attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee76691",
   "metadata": {},
   "source": [
    "모든 토큰을 다음과 같이 토큰 ID로 변환한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7270e17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T14:42:24.088155Z",
     "start_time": "2022-11-14T14:42:24.082437Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1045, 2293, 4420, 102, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfae639",
   "metadata": {},
   "source": [
    "`token_ids` 와 `attention_mask` 를 텐서로 변환한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd89ebf6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T14:42:24.449475Z",
     "start_time": "2022-11-14T14:42:24.441056Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101, 1045, 2293, 4420,  102,    0,    0]]) tensor([[1, 1, 1, 1, 1, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.tensor(token_ids).unsqueeze(0)\n",
    "attention_mask = torch.tensor(attention_mask).unsqueeze(0)\n",
    "\n",
    "print(token_ids, attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f870b06",
   "metadata": {},
   "source": [
    "**임베딩 추출하기**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332d35c3",
   "metadata": {},
   "source": [
    "모델은 두 값으로 구성된 튜플로 출력을 반환한다. 첫 번째 값은 은닉 상태 표현인데, 이는 최종 인코더(12번째 인코더)에서 얻은 모든 토큰의 표현 벡터로 구성되어 있고, 두 번째 값은 [CLS] 토큰의 표현으로 구성된다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac6f276e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T14:42:25.080226Z",
     "start_time": "2022-11-14T14:42:24.977376Z"
    }
   },
   "outputs": [],
   "source": [
    "outputs = model(token_ids, attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22b02fa",
   "metadata": {},
   "source": [
    "`hidden_rep` 는 입력에 대한 모든 토큰의 임베딩(표현)을 포함한다.  \n",
    "`[1, 7, 768]`는 `[batch_size, sequence_length, hidden_size]` 를 의미한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f926afcf",
   "metadata": {},
   "source": [
    "- `hidden_rep[0][0]` 은 첫 번째 토큰인 `[CLS]` 의 표현 벡터를 제공한다. \n",
    "- `hidden_rep[0][1]` 은 두 번째 토큰인 `I` 의 표현 벡터를 제공한다. \n",
    "- `hidden_rep[0][2]` 는 세 번째 토큰인 `love` 의 표현 벡터를 제공한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4893ca68",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T14:42:25.537785Z",
     "start_time": "2022-11-14T14:42:25.531796Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7, 768])\n"
     ]
    }
   ],
   "source": [
    "hidden_rep = outputs.last_hidden_state\n",
    "print(hidden_rep.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e141936",
   "metadata": {},
   "source": [
    "`cls_head` 에는 `[CLS]` 토큰의 표현이 포함된다.  \n",
    "크기 `[1, 768]` 은 `[batch_size, hidden_size]` 를 나타낸다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43027b7d",
   "metadata": {},
   "source": [
    "`cls_head` 가 문장 전체의 표현을 보유하고 있다는 것을 배웠으므로 `cls_head` 를 `I love Korea` 문장의 표현 벡터로 사용할 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48a49c81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T14:42:26.095824Z",
     "start_time": "2022-11-14T14:42:26.092623Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "cls_head = outputs.pooler_output\n",
    "print(cls_head.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd47b6a1",
   "metadata": {},
   "source": [
    "# BERT의 모든 인코더 레이어에서 임베딩을 추출하는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce90e4b",
   "metadata": {},
   "source": [
    "모든 인코더 레이어에서 임베딩을 얻기 위해 사전 학습된 BERT 모델을 다운로드할 때 `output_hidden_states = True` 로 설정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e0a35ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T14:42:29.219059Z",
     "start_time": "2022-11-14T14:42:26.677926Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26946348",
   "metadata": {},
   "source": [
    "앞에서 본 문장을 그대로 토큰화하고 시작 부분에 `[CLS]` 토큰을 추가하고 끝에 `[SEP]` 토큰을 추가한다.  \n",
    "토큰의 길이를 7로 유지해야 한다고 가정하며 `[PAD]` 토큰을 추가한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c0ec388",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T14:42:29.226947Z",
     "start_time": "2022-11-14T14:42:29.221338Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'i', 'love', 'korea', '[SEP]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "sentence = 'I love Korea'\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
    "tokens = tokens + ['[PAD]'] + ['[PAD]']\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba72d3c9",
   "metadata": {},
   "source": [
    "어텐션 마스크를 정의한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0ede1e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T14:42:29.231795Z",
     "start_time": "2022-11-14T14:42:29.228967Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "attention_mask = [1 if i!= '[PAD]' else 0 for i in tokens]\n",
    "print(attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fe1017",
   "metadata": {},
   "source": [
    "토큰을 토큰 ID로 변환한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6be37631",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T14:42:29.236789Z",
     "start_time": "2022-11-14T14:42:29.234219Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1045, 2293, 4420, 102, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4eb7fc",
   "metadata": {},
   "source": [
    "`token_ids` 와 `attention_mask` 를 텐서로 변환한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95838658",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T14:42:29.242775Z",
     "start_time": "2022-11-14T14:42:29.239064Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101, 1045, 2293, 4420,  102,    0,    0]]) tensor([[1, 1, 1, 1, 1, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.tensor(token_ids).unsqueeze(0)\n",
    "attention_mask = torch.tensor(attention_mask).unsqueeze(0)\n",
    "\n",
    "print(token_ids, attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8069f71f",
   "metadata": {},
   "source": [
    "모델은 다음과 같이 3개의 값이 있는 튜플을 반환한다. \n",
    "- 첫 번째 값인 `last_hidden_state` 는 최종 인코더 계층(12번째 인코더)에서만 얻은 모든 토큰의 표현을 가진다. \n",
    "- `pooler_output` 은 최종 인코더 계층의 `[CLS]` 토큰 표현을 나타내며 선형 및 tanh 활성화 함수에 의해 계산된다. \n",
    "- `hidden_states` 는 모든 인코더 계층에서 얻은 모든 토큰의 표현을 포함한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "711ea192",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T14:42:29.294789Z",
     "start_time": "2022-11-14T14:42:29.244271Z"
    }
   },
   "outputs": [],
   "source": [
    "outputs = model(token_ids, attention_mask)\n",
    "\n",
    "last_hidden_state = outputs.last_hidden_state\n",
    "pooler_output = outputs.pooler_output\n",
    "hidden_states = outputs.hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f44d1d8",
   "metadata": {},
   "source": [
    "`last_hidden_state` 는 최종 인코더 계층(12번째 인코더)에서만 얻은 모든 토큰의 표현을 가지고 있다.  \n",
    "`[1, 7, 768]`는 `[batch_size, sequence_length, hidden_size]` 를 의미한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ef44b7",
   "metadata": {},
   "source": [
    "- `hidden_rep[0][0]` 은 첫 번째 토큰인 `[CLS]` 의 표현 벡터를 제공한다. \n",
    "- `hidden_rep[0][1]` 은 두 번째 토큰인 `I` 의 표현 벡터를 제공한다. \n",
    "- `hidden_rep[0][2]` 는 세 번째 토큰인 `love` 의 표현 벡터를 제공한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c3dbdff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T14:42:29.300454Z",
     "start_time": "2022-11-14T14:42:29.296165Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 768])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda1845a",
   "metadata": {},
   "source": [
    "`pooler_output` 은 최종 인코더 계층의 `[CLS]` 토큰 표현을 포함하고 있으며 선형 및 tanh 활성화 함수에 의해 계산된다.  \n",
    "`[1, 768]` 은 `[batch_size, hidden_size]` 를 나타낸다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20e72401",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T14:42:29.861302Z",
     "start_time": "2022-11-14T14:42:29.853062Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooler_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c7f30c",
   "metadata": {},
   "source": [
    "마지막으로 모든 인코더 계층에서 얻은 모든 토큰의 표현을 포함하는 `hidden_states` 가 있다.  \n",
    "이는 입력 임베딩 레이어($h_0$)에서 최종 인코더 레이어($h_12$)까지 모든 인코더 레이어의 표현을 포함하는 13개의 값을 포함하는 튜플이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "46e8becd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T14:42:30.248709Z",
     "start_time": "2022-11-14T14:42:30.244434Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hidden_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e2e1ab",
   "metadata": {},
   "source": [
    "- `hidden_states[0]` 는 입력 임베딩 레이어 $h_0$ 에서 얻은 모든 토큰의 표현 벡터를 가진다. \n",
    "- `hidden_states[1]` 는 첫 번째 인코더 계층 $h_1$ 에서 얻은 모든 토큰의 표현 벡터를 가진다. \n",
    "- `hidden_states[2]` 는 두 번째 인코더 계층 $h_2$ 에서 얻은 모든 토큰의 표현 벡터를 가진다. \n",
    "- `hidden_states[12]` 는 최종 인코더 레이어 $h_12$ 에서 얻은 모든 토큰의 표현 벡터를 가진다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c75fa24",
   "metadata": {},
   "source": [
    "`[1, 7, 768]`는 `[batch_size, sequence_length, hidden_size]` 를 의미한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1fb62b07",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T14:42:31.209290Z",
     "start_time": "2022-11-14T14:42:31.202266Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 768])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f759d21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
